{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d6J7gvjNHVrc",
        "outputId": "4bfa67c2-2a12-4065-decc-32fb92903041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.60)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.81.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (2.11.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.18-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_core, langchain_openai, langchain_community\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.60\n",
            "    Uninstalling langchain-core-0.3.60:\n",
            "      Successfully uninstalled langchain-core-0.3.60\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 langchain_core-0.3.63 langchain_openai-0.3.18 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain_openai langchain_community langchain_core -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3PsHM8hHHVrf",
        "outputId": "50a637cd-65e5-4ec3-d6e3-0ffb38eaf2f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "甘肃天水的麻辣烫作为一种地方特色小吃，其流行的时间并不是一个确切的日期，而是逐渐在一定时间段内被更多人所熟知和喜爱的过程。天水麻辣烫之所以能够走红，主要归因于以下几个方面：\n",
            "\n",
            "1. **独特的风味**：天水麻辣烫以其特有的辣而不燥、麻而不过的特点著称，这种独特的口味满足了广大食客对美食的新鲜感和好奇心。\n",
            "\n",
            "2. **文化背景**：天水作为甘肃省的一个重要城市，拥有悠久的历史文化和丰富的民族风情。这些因素使得天水麻辣烫不仅仅是一种食物，更承载了一定的文化意义，吸引了众多游客前来品尝。\n",
            "\n",
            "3. **社交媒体的推动**：随着互联网尤其是社交媒体的发展，许多美食博主、网红通过视频、图文等形式分享自己尝试天水麻辣烫的经历，极大地提高了这一特色小吃的知名度和吸引力。\n",
            "\n",
            "4. **旅游经济的影响**：近年来，随着旅游业的发展，越来越多的人选择前往甘肃旅游。天水作为一个旅游热点城市，其特色美食自然也受到了广泛关注。\n",
            "\n",
            "5. **品质与服务**：除了上述外部因素外，天水当地商家对于食材的选择、烹饪技艺的提升以及服务质量的改善也是促使天水麻辣烫越来越受欢迎的重要原因。\n",
            "\n",
            "综上所述，天水麻辣烫之所以能火起来，是多方面因素共同作用的结果。随着时间的推移，相信会有更多的人了解并爱上这道美味的地方特色小吃。\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "question = \"甘肃天水麻辣烫是什么时候火的，为什么会火\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"qwen2.5-72b-instruct\", temperature=0)\n",
        "\n",
        "res = llm.invoke([\n",
        "    SystemMessage(\"你是一个智能助手\"),\n",
        "    HumanMessage(question)\n",
        "])\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   获取外界知识\n",
        "\n",
        "与让LLM拥有记忆原理相似，让LLM获取外界知识，原理就是通过一些工具获取互联网中实时信息，作为上下文来回答。langchain中帮我们封装了很多工具来获取外界知识，详见：https://python.langchain.com/v0.2/docs/integrations/tools/。\n",
        "而demo中将使用Tavily Search来演示，大家进入官网Tavily使用google账号申请即可，每月会有1000次免费额度，足够日常开发调试与学习。\n",
        "\n",
        "API 获取地址： https://app.tavily.com/home\n",
        "\n",
        "调用方法：https://python.langchain.com/v0.2/docs/integrations/tools/tavily_search/\n"
      ],
      "metadata": {
        "id": "b_h4ynnsIAEJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sd5E3A-rHVrg",
        "outputId": "99a03eeb-2e94-407f-dfa3-7f49c18450e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 甘肃天水麻辣烫的走红时间及原因\n",
            "\n",
            "#### 走红时间\n",
            "甘肃天水麻辣烫的走红始于2024年2月13日。当天，天水网友“一杯梁白开”在短视频平台发布了一段关于天水麻辣烫的视频，视频中麻辣烫色泽诱人、热辣滚烫，短时间内获得了百万点赞。随后，这条视频引发了大量网民的关注和转发，迅速在网络上形成了热议。\n",
            "\n",
            "#### 走红原因\n",
            "1. **短视频平台的推动**：\n",
            "   - **用户生成内容**：最初是一条普通用户的短视频，因其真实性和吸引力迅速传播。\n",
            "   - **网络效应**：视频被大量转发，形成了几何倍数的增长，最终在全国范围内形成了热潮。\n",
            "\n",
            "2. **美食的独特性**：\n",
            "   - **食材的特殊性**：天水麻辣烫使用当地特有的甘谷辣椒和秦安花椒，这些食材赋予了麻辣烫独特的风味。\n",
            "   - **制作工艺**：天水麻辣烫的汤底由多种香料熬制而成，调料配比独特，使得其味道更加鲜美。\n",
            "\n",
            "3. **年轻人群体的影响力**：\n",
            "   - **年轻人的偏好**：年轻人喜欢尝试新鲜事物，尤其是美食。天水麻辣烫的麻辣鲜香符合年轻人的口味。\n",
            "   - **社交媒体的传播**：年轻人是社交媒体的主要用户，他们的分享和推荐加速了天水麻辣烫的传播。\n",
            "\n",
            "4. **城市文化的宣传**：\n",
            "   - **地方政府的支持**：天水市政府积极推广本地美食，开通“麻辣烫公交专线”，提供免费停车等服务，吸引了更多游客。\n",
            "   - **文化背景**：天水作为历史文化名城，拥有丰富的文化底蕴，麻辣烫的走红也为城市文化宣传提供了新的契机。\n",
            "\n",
            "5. **差异化竞争**：\n",
            "   - **地域特色**：天水麻辣烫与其他城市的麻辣烫有所不同，具有明显的地域特色，吸引了对新鲜事物感兴趣的游客。\n",
            "   - **市场空白**：此前西北地区尚未有城市在短视频平台上走红，天水填补了这一空白，满足了人们“看四方”的心理需求。\n",
            "\n",
            "### 结论\n",
            "天水麻辣烫的走红是多种因素综合作用的结果。短视频平台的推动、美食的独特性、年轻人群体的影响力、城市文化的宣传以及差异化竞争，共同促成了天水麻辣烫的火爆。这一现象不仅带火了天水的旅游业，也促进了当地经济的发展。\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "question = \"甘肃天水麻辣烫是什么时候火的，为什么会火\"\n",
        "\n",
        "tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        ")\n",
        "\n",
        "search = tool.invoke({\"query\": question})\n",
        "\n",
        "llm = ChatOpenAI(model=\"qwen2.5-72b-instruct\", temperature=0)\n",
        "\n",
        "res = llm.invoke([\n",
        "    SystemMessage(\"你是一个智能助手，请根据上下文回答问题，上下文是:\" + \"\\n\".join([x['content'] for x in search])),\n",
        "    HumanMessage(question)\n",
        "])\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"你是一个智能助手，请根据上下文回答问题，上下文是:\" + \"\\n\".join([x['title'] for x in search]))"
      ],
      "metadata": {
        "id": "1HUfq76WJTEp",
        "outputId": "4df30b24-357a-481e-8808-68c1d63f5650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你是一个智能助手，请根据上下文回答问题，上下文是:天水遇上麻辣烫：美食这样带火一座城 - 光明网\n",
            "8000年的甘肃天水，火不过一碗麻辣烫_澎湃号·湃客_澎湃新闻-The Paper\n",
            "天水麻辣烫出圈背后的底层逻辑_经济频道_央视网(cctv.com)\n",
            "为什么突然火的是天水麻辣烫？ - 知乎\n",
            "一碗麻辣烫竟带火一座城，甘肃天水逆袭出圈的秘诀到底在哪儿？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以看出，借助网络，我们是能够让大模型了解实事的，不然，它的知识信息只停留在它发布的那刻之前学到的。\n",
        "我们多试几个例子，就会发现，获取外界知识会让大模型的回答更加准确"
      ],
      "metadata": {
        "id": "NIegFtBVKXo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LuAEYpfrHVrg",
        "outputId": "b608f456-2dcc-42be-c53f-a221cb5041bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'title': '如何看待2024年7月13日特朗普被枪击，耳朵有血迹？ - 知乎', 'url': 'https://www.zhihu.com/question/661562795', 'content': '多家美国媒体消息，当地时间13日下午，特朗普在美国宾夕法尼亚州举行竞选集会发表演讲时，现场响起枪声。 ... 唐纳德·特朗普在宾夕法尼亚州巴特勒举行了一场竞选集会，就在这个时候，一名枪手向特朗普的讲台开了几枪。 ... 根据警方说：特朗普没有被', 'score': 0.64128816}, {'title': '现场发生了什么？多图直击特朗普竞选集会现场枪击事件', 'url': 'https://news.qq.com/rain/a/20240714A027H400', 'content': '综合新华社、央视新闻消息，当地时间7月13日下午，前美国总统唐纳德·特朗普在美国宾夕法尼亚州举行的竞选集会上发表演讲时，右耳被一枚子弹', 'score': 0.6307082}, {'title': '特朗普競選集會遭槍擊畫面顯示其染血受傷－ BBC News 中文', 'url': 'https://www.youtube.com/watch?v=25fn5P5Ukgg', 'content': '美國前總統特朗普週六（7月13日）在賓夕法尼亞州舉行的競選集會發生槍擊案。現場畫面顯示，特朗普在演講時突然傳出槍響。他被緊急送下舞台，耳朵周圍流血', 'score': 0.604703}, {'title': '7·13美国总统选举枪击事件 - 百度百科', 'url': 'https://baike.baidu.com/item/7·13美国总统选举枪击事件/64649663', 'content': '当地时间2024年7月20日，前白宫医生 罗尼·杰克逊 就美国前总统特朗普遭刺杀未遂后的恢复情况发布声明称，打中特朗普右耳的子弹距离他的脑袋不足四分之一英寸（0.635厘米），子弹造成了一个2厘米宽的伤口，一直延伸到耳朵的软骨表面。最初有大量出血', 'score': 0.5855047}, {'title': 'Fbi确认!特朗普在\"未遂刺杀\"中被子弹击中 - 新浪财经', 'url': 'https://finance.sina.com.cn/jjxw/2024-07-27/doc-incfpsme4478331.shtml', 'content': '7月13日，特朗普在宾夕法尼亚州举行竞选集会时遭到\"未遂刺杀\"，右耳受伤。 23日，美国特勤局局长金伯莉·奇特尔宣布辞职，原因是特勤局未能', 'score': 0.5806618}]\n",
            "----------------分割线----------------\n",
            "2024年1月23日，前总统唐纳德·特朗普在南卡罗来纳州的一次集会上发表演讲时，一名观众投掷的水瓶击中了他的耳朵。这一事件引发了现场的混乱，但特朗普继续完成了他的演讲。事后，特朗普的竞选团队表示他没有受伤。\n",
            "----------------分割线----------------\n",
            "特朗普是在2024年7月13日，在宾夕法尼亚州举行的竞选集会上发表演讲时遭到袭击，右耳被子弹打中。\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools import TavilySearchResults\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "\n",
        "question = \"特朗普是几年几月几日在演讲中被袭击打中耳朵\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"qwen2.5-72b-instruct\", temperature=0)\n",
        "\n",
        "tool = TavilySearchResults(\n",
        "    max_results=5,\n",
        "    search_depth=\"advanced\",\n",
        "    include_answer=True,\n",
        "    include_raw_content=True,\n",
        ")\n",
        "\n",
        "search = tool.invoke({\"query\": question})\n",
        "\n",
        "res = llm.invoke([\n",
        "    SystemMessage(\"你是一个智能助手\"),\n",
        "    HumanMessage(question)\n",
        "])\n",
        "\n",
        "res2 = llm.invoke([\n",
        "    SystemMessage(\"你是一个智能助手，请根据上下文回答问题，上下文是:\" + \"\\n\".join([x['content'] for x in search])),\n",
        "    HumanMessage(question)\n",
        "])\n",
        "\n",
        "print(search)\n",
        "\n",
        "print(\"----------------分割线----------------\")\n",
        "\n",
        "print(res.content)\n",
        "\n",
        "print(\"----------------分割线----------------\")\n",
        "\n",
        "print(res2.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py3110",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}